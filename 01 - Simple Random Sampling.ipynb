{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7b374d-bb36-484a-b948-cb2300952c19",
   "metadata": {},
   "source": [
    "# 1 - Simple Random Sampling\n",
    "\n",
    "In the realm of statistics, when we're faced with a vast expanse of data - like a detailed thematic map representing diverse geographical features - it's often impractical or even impossible to examine every single point. This is where the elegance of sampling techniques comes into play.\n",
    "\n",
    "Simple Random Sampling (SRS), as the name suggests, is the most straightforward of these techniques.  It's akin to drawing names out of a hat - every single point in our 'map' (or dataset) has an equal chance of being selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37485bc3-52e9-4df4-bf6c-c32f24878504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ee5b5-a200-43e0-afd8-af1623858751",
   "metadata": {},
   "source": [
    "## 1 - Create a fake image with a predefined proportion of \"change\"\n",
    "\n",
    "In this initial cell, we're essentially creating a simplified representation of a geographical area, much like a digital map. The code accomplishes this by generating a two-dimensional array (think of it as a grid or matrix) filled with random numbers.\n",
    "\n",
    "A certain percentage of those pixels will be given the value 1 for change, and 0 for no change. The *proportion_of_change* variable will define how many points are going to be 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3734056-4c50-4730-bea8-3ed4c3796a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small helper function to create a fake image of 1s and 0s\n",
    "def create_random_array(size, proportion, shuffle=True):\n",
    "\n",
    "    # create an array of size2\n",
    "    array = np.zeros(size*size)\n",
    "    change_samples = int(size * size * proportion)\n",
    "    \n",
    "    # attribute a proportion of change\n",
    "    array[:change_samples] = 1  # Assign rows to stratum 1\n",
    "    \n",
    "    # shuffle array\n",
    "    if shuffle:\n",
    "        np.random.shuffle(array)\n",
    "    return array.reshape(size, size)\n",
    "\n",
    "\n",
    "proportion_of_change = 0.01  # proportion of change pixels within the image (expressed as fraction)\n",
    "size = 250                   # one-sided size\n",
    "change_array = create_random_array(size, proportion_of_change)\n",
    "\n",
    "# plot image\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.imshow(change_array, interpolation='nearest', cmap='RdYlGn_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57721c-9dd9-4b82-b3ec-5a04a0646bfd",
   "metadata": {},
   "source": [
    "## 2 - Estimation of change proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3cfb6-b835-47d6-8d2e-fbad0f1686aa",
   "metadata": {},
   "source": [
    "### 2.1 - A single estimate through simple random sampling\n",
    "\n",
    "In the first part of this cell, we calculate the true proportion of change from our array. In practise, this is not possible, as it would mean to have a perfect map or knowledge of the total population, i.e. each pixel's true value. Since satellite-based maps are hugely large and prone to some degree of error, this is impossible. \n",
    "\n",
    "In the second part, we sample a subset of the array. Based on the subset we calculate the proportion of change, which becomes our estimate.\n",
    "\n",
    "If you run this cell a couple of times, you will notice that the estimate is constantly changing. This is because each time a different subset is selected, which affects the estimated proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc5b26-39e7-426f-a4ea-6beeecbcbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "print('Number of change samples:', np.sum(change_array))\n",
    "true_proportion = np.sum(change_array)/change_array.size\n",
    "print(f'{true_proportion} is our true proportion of change')\n",
    "\n",
    "# this line randomly selects the number of samples defiend above without replacement\n",
    "sampled = np.random.choice(change_array.flatten(), sample_size, replace=False)\n",
    "\n",
    "\n",
    "estimated_proportion = sampled.sum()/len(sampled)\n",
    "print(f'{estimated_proportion} is our estimated proportion of change')      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd59d1-7456-46d9-90df-47c4a0e39196",
   "metadata": {},
   "source": [
    "### 2.2 - Sampling variability and experimental proof of unbiasedness \n",
    "\n",
    "In this cell, we run the sampling exercise 10000 times. Each time, a different sample is taken from the full array. Therefore, each estimate is slightly different depending on how much change points are part of that sample. This is subject to the random selection and is known as **sampling variability**. Because of the random selection of a subset from your total population, a single estimate might be above or below your true value. However, if this process is repeated a lot of times, the mean of the estimates equals the true value - which means the estimator is **unbiased**.  \n",
    "\n",
    "All estimates are stored in a DataFrame at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2151be2-a90d-4661-b7bf-db848dfbb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a dicitonary to be filled through out the loop\n",
    "d = {}\n",
    "for a in range(10000):\n",
    "    \n",
    "    # setting a random seed according to the loop's number of iteration\n",
    "    np.random.seed(seed=a)\n",
    "\n",
    "    # randomly sample the array created above\n",
    "    sampled = np.random.choice(change_array.flatten(), sample_size, replace=False)\n",
    "    \n",
    "    # calculate the proportion of change based on the samples\n",
    "    d[a] = [sampled.sum()/len(sampled)]\n",
    "\n",
    "# structure data into a dataframe table\n",
    "df = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "df.columns = ['Run', 'Estimate']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165444f-33b5-41f5-8c52-a24949eb6588",
   "metadata": {},
   "source": [
    "By plotting the distribution of all 10000 runs, we will notice that the mean of those runs and the true proportion (dotted red line) is exactly the same. This can be seen as an experimental proof that the applied logic is unbiased.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608f3ad-7985-44dc-a15f-a6c41c0f2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "# plot the distribution of the Estimates as a Violinplot\n",
    "ax = sns.violinplot(df, y='Estimate', ax=ax, alpha=0.5)\n",
    "plt.axhline(true_proportion, c='red', linestyle=':')\n",
    "\n",
    "# Add legend for true value\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "point = Line2D([0], [0], label='True value', linestyle=':', c='r')\n",
    "handles.append(point) \n",
    "ax.legend(handles=handles)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed46c05-5a79-4d25-908a-e879e683fadf",
   "metadata": {},
   "source": [
    "### 2.3 - Sampling variability as a function of sample size\n",
    "\n",
    "The more samples are used to derive an estimate of the proportion, the more likely it is to be closer to the true value, meaning the sampling variability is reduced. In the follwoing cell, we repeat the previous experiment, but adding different sample sizes. It can be seen that with a larger amount of samples the estimates are closer to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d915e-50d1-4d60-b408-40374cdb719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [1000, 3000, 5000, 7000, 9000]\n",
    "iterations = 10000 # nr. of iterations per sample size\n",
    "\n",
    "# initialize ariables to be used in the loop\n",
    "d, a = {}, 0\n",
    "for i in sample_sizes:\n",
    "    for j in range(iterations):\n",
    "\n",
    "        # set random seed according to the second loop's number of iteration\n",
    "        np.random.seed(seed=j)\n",
    "\n",
    "        # randomly sample the above created array with the sample size i from the first loop\n",
    "        sampled = np.random.choice(change_array.flatten(), i, replace=False)\n",
    "        \n",
    "        # add the samplesize and calculated proportion of change to the dictionary for that specific loop\n",
    "        d[a] = [i, sampled.sum()/len(sampled)]\n",
    "        a += 1\n",
    "        \n",
    "# structure the results in a dataframe\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df.columns = ['Sample Size', 'Estimate']\n",
    "#display(df.head())\n",
    "\n",
    "# plot the distribution of the Estimates as a Violinplot, grouped by sample size\n",
    "fig, ax = plt.subplots(1)\n",
    "ax = sns.violinplot(df, y='Estimate', x='Sample Size', hue='Sample Size', ax=ax, palette='magma', legend=False)\n",
    "\n",
    "# Add legend for true value\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "point = Line2D([0], [0], label='True value', linestyle=':', c='r')\n",
    "handles.append(point) \n",
    "ax.legend(handles=handles)\n",
    "\n",
    "plt.axhline(true_proportion, c='red', linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdc604-182a-4849-90bf-85c7ad31d6f3",
   "metadata": {},
   "source": [
    "## 3 - Uncertainty estimation\n",
    "\n",
    "Sampling variability belongs to the nature of sampling. While we do get a single value for our estimate, we do not know how close it is to the true value. Depending on the proportion of the phenomena among the total population and the sample size, it is possible to derive an unbiased estimate of uncertainty. Uncertainty is usually expressed as a confidence interval, and it tells us how likely our estimate can be found in a certain range around our estimate.  \n",
    "\n",
    "The unbiasedness of the uncertainty estimate can be demonstrated. The standard deviation of a number of experimental runs equals the mean of the calculated standard error using the estimator. \n",
    "In this sense, the uncertainty expresses the range of the sampling variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a379f28-13ac-4d51-9371-65b8055a40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_random_sampling(array, sample_size, seed=42):\n",
    "\n",
    "    # set random seed according to given seed\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    N_h = change_array.size \n",
    "    n_h = sample_size\n",
    "    \n",
    "    # randomly sample the given array with the given sample size\n",
    "    sampled = np.random.choice(change_array.flatten(), n_h, replace=False)\n",
    "\n",
    "    # calculate the proportion\n",
    "    tau_hat = sampled.sum()/len(sampled)\n",
    "    \n",
    "    # calculate variance\n",
    "    S = tau_hat * (1-tau_hat)\n",
    "\n",
    "    # calculate finite correction\n",
    "    finite = 1 - n_h/N_h\n",
    "\n",
    "    # calculate the standard error\n",
    "    se = np.sqrt(finite * S/(n_h - 1))\n",
    "\n",
    "    return sample_size, tau_hat, se\n",
    "\n",
    "\n",
    "d, a = {}, 0\n",
    "for i in sample_sizes:\n",
    "    for j in range(iterations):\n",
    "        d[a] = simple_random_sampling(change_array, i, a)\n",
    "        a += 1\n",
    "\n",
    "# structure the results in a dataframe\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df.columns = ['Sample Size', 'Estimate', 'SE']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a0acd-0b9c-42df-88b6-de70127af1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the Estimates as a Violinplot, grouped by sample size\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,8))\n",
    "\n",
    "# first plot ( as above) with the distribution of estimates and true value\n",
    "ax[0] = sns.violinplot(df, y='Estimate', x='Sample Size', hue='Sample Size', ax=ax[0], palette='magma', legend=False)\n",
    "ax[0].axhline(true_proportion, c='red', linestyle=':')\n",
    "ax[0].set_title('Distribution of the estimated proportion for different sampling sizes')\n",
    "\n",
    "# Add legend for true value\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "point = Line2D([0], [0], label='True value', linestyle=':', c='r')\n",
    "handles.append(point) \n",
    "ax[0].legend(handles=handles)\n",
    "\n",
    "# second plot with distribution of estimated SE's and calculated one (red dots)\n",
    "ax[1] = sns.violinplot(df, y='SE', x='Sample Size', hue='Sample Size', ax=ax[1], palette='magma', legend=False)\n",
    "ax[1] = sns.pointplot(df.groupby('Sample Size').std().reset_index(), x='Sample Size', y='Estimate', ax=ax[1], c='red', markersize=1.5, linestyle=' ')\n",
    "ax[1].set_title('Distribution of the estimated SE for different sampling sizes')\n",
    "\n",
    "# Add legend for red dots\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "point = Line2D([0], [0], label='Calculated SE', marker='.', markersize=10, markerfacecolor='r', markeredgecolor='r', linestyle='')\n",
    "handles.append(point) \n",
    "ax[1].legend(handles=handles)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
